{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering II: Column Transformers and Pipelines\n",
    "<img src = \"./images/lego.webp\" width = \"450\">\n",
    "\n",
    "<a href = \"https://www.highsnobiety.com/p/lego-transformers-optimus-prime/\">Image Source</a>\n",
    "\n",
    "This notebook build on the [feature engineering introduction notebook](1.1_intro_to_fe.ipynb) to automate the transformation process, simplifying our workflow and unlocking the potential of the sklearn library.\n",
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penguin Dataset\n",
    "\n",
    "We will use the Palmer Penguin Dataset.\n",
    "\n",
    "### Business Goal\n",
    "> Predict the penguin body mass given the input feature : flipper_length_mm, bill_length_mm, species and sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization stack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# machine-learning stack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    MinMaxScaler,\n",
    "    KBinsDiscretizer,\n",
    "    PolynomialFeatures\n",
    ")\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# miscellaneous\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/penguins.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    'flipper_length_mm',\n",
    "    'bill_length_mm'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'species',\n",
    "    'sex'\n",
    "]\n",
    "\n",
    "features = numerical_features + categorical_features\n",
    "\n",
    "target_variable = 'body_mass_g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature-Target separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix \n",
    "X = df[features]\n",
    "\n",
    "# Target column\n",
    "y = df[target_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=88, shuffle=True, stratify=X['species'])\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "Check issues with data:\n",
    "+ which variable has missing values?\n",
    "+ which variables are binary, categorical, metric?\n",
    "+ do categorical variables have non-numeric values?\n",
    "+ do metric features are varying on a different scale?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train is a DataFrame and y_train is a Series\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "print(\"Combined train data shape:\", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We have a pair of tools, `ColumnTransformer()` and `Pipeline()`, which can dramatically simplify and automate feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ColumnTransformer()\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\">`ColumnTransformer` </a> allows us to specify which columns receive which transformations (and conveniently reintegrates the dataset).\n",
    "\n",
    "Parameters:\n",
    " * `transformers` - list of tuples `(name, transformer, columns)`\n",
    "\n",
    " * `remainder` - used as last tuple if there are any untouched columns. Choose either `drop` or `passthrough`<br></br>\n",
    "  \n",
    ">**Note** that `ColumnTransformer()` runs all transformers in parallel, not sequentially, so if a column is transformed more than once, the version generated by each of these transformations will be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our first transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our transformers - name, method, target\n",
    "transformers = [('ohe', OneHotEncoder(drop = 'first',sparse_output=False), ['species', 'sex']),\n",
    "                ('bill_scaler', RobustScaler(), [['bill_length_mm', 'flipper_length_mm']]),\n",
    "                #('flip_scaler', RobustScaler(), ['flipper_length_mm'])\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we instantiate our ColumnTransformer() object\n",
    "column_transformer = ColumnTransformer(transformers,\n",
    "                                       remainder = 'drop')\n",
    "column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to impute missing values in sex and flipper_length_mm, but if we do so in this transformer we will create an imputed copy of sex and flipper_length_mm and a one-hot encoded versions with missing values.\n",
    "\n",
    "What we need here is a way to sequentially apply transformations, which leads us nicely into..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">`Pipeline()`</a> allows us to sequentially apply multiple transformers on the same column(s).\n",
    "\n",
    "\n",
    "Parameters:\n",
    " * `steps` - list of tuples `(name, transformer)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a pipeline and integrate it into our transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the steps to impute and transform sex\n",
    "sex_steps = [('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "             ('sex_ohe', OneHotEncoder(drop = 'first',sparse_output=False))\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the sex pipeline\n",
    "sex_pipeline = Pipeline(steps=sex_steps)\n",
    "sex_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the steps to impute and transform flipper_length_mm\n",
    "flipper_steps = [('imputer', SimpleImputer(strategy = 'median')),\n",
    "             ('flipper_scaler', RobustScaler())\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the flipper pipeline\n",
    "flipper_pipeline = Pipeline(steps=flipper_steps)\n",
    "flipper_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a new transformer to include this pipeline\n",
    "transformers_2 = [('sex_pipeline', sex_pipeline, ['sex']),\n",
    "                  ('ohe', OneHotEncoder(drop = 'first',sparse_output=False), ['species']),\n",
    "                  ('flipper_pipeline', flipper_pipeline, ['flipper_length_mm']),\n",
    "                ('scaler',RobustScaler(), ['bill_length_mm'])\n",
    "                 ]\n",
    "\n",
    "column_transformer_2 = ColumnTransformer(transformers=transformers_2,\n",
    "                                         remainder = 'drop').set_output(transform='pandas')\n",
    "column_transformer_2      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Try It Out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the column transformer object ONLY using train data\n",
    "column_transformer_2.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "X_train_fe = column_transformer_2.transform(X_train)\n",
    "X_train_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the variables have been tranformed according to the strategies defined in our pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesting Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen how we can use essentially any named function or object as a step in our pipelines and transformers. The final trick we'll explore with pipelines is the ability to nest several layers within one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a pipeline containing our complete transformer and then a linear regression model\n",
    "model_steps = [('feature_enginnering', column_transformer_2),\n",
    "               ('linear_regression', LinearRegression())]\n",
    "linear_model = Pipeline(steps = model_steps)\n",
    "linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = linear_model.score(X_train,y_train)\n",
    "print(f\"training r2 score: {round(training_score, 6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Weigths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_step = linear_model.steps[0][1]\n",
    "column_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = linear_model.steps[1][1]\n",
    "model_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_model = pd.DataFrame(data=model_step.coef_.reshape(1,-1), columns=column_step.get_feature_names_out(), index=['weigth'])\n",
    "\n",
    "coef_model['intercept'] = model_step.intercept_\n",
    "coef_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = linear_model.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = linear_model.score(X_test,y_test)\n",
    "print(f\"test r2 score: {round(test_score, 6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">By applying a pipeline to our test data, we ensure that the test data is treated the in exact same way as the data the model was trained on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
