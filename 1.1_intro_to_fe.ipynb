{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d145d3f5",
   "metadata": {},
   "source": [
    "# Feature Engineering Intro I\n",
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7514f60",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48037d4f",
   "metadata": {},
   "source": [
    "What is our __goal__ when we __train a machine learning model__ (ML)?\n",
    "\n",
    "<img src = \"./images/basic_model.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f042b929",
   "metadata": {},
   "source": [
    "The primary goal when training a machine learning model is to develop a system that can accurately make predictions or decisions based on input data. \n",
    "But how do we improve the accuracy of our predictions?\n",
    "\n",
    "**Selecting the right model** is essential, but it's only the beginning. Beyond model selection, we must explore other strategies to refine our model's learning, resulting in more **accurate predictions**.\n",
    "\n",
    "Consider the learning process of the model:\n",
    "\n",
    "When provided with `X_train` input, the algorithm determines the optimal parameters to align the model‚Äôs output with the known `y_train` values.\n",
    "\n",
    "To elevate our model from just okay to truly effective, we must actively support its learning process.\n",
    "\n",
    "So, what strategies can we employ to achieve this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5172642",
   "metadata": {},
   "source": [
    "## What is Feature Engineering?\n",
    "The diagram below is **over-simplified**. Simply feeding our algorithm with all the raw data results in a model that is equally raw and messy. Instead, we need to **clean up** the data and make **creative decisions** to select the **key features** that will enable the model to accurately predict outcomes.\n",
    "\n",
    "<img src = \"./images/feature_eng.png\" width=500>\n",
    "\n",
    "Some aspects of **feature engineering** are methodical and consistent. We'll explore these first.\n",
    "\n",
    "The other aspects are more similar to an **art form**, requiring a deep understanding of the subject and a bit of **human intuition**.\n",
    "\n",
    "The process of **feature engineering** can be one of the most **time-consuming** parts of modeling, but it's essential. Without it, your model will struggle to discern patterns amidst the noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892fad6",
   "metadata": {},
   "source": [
    "**Estimated time spent with data organizing**\n",
    "\n",
    "<img src = \"./images/stacked-chart.jpeg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474ee23",
   "metadata": {},
   "source": [
    "### Feature engineering techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8af094",
   "metadata": {},
   "source": [
    " |       technique      |                                        usefulness                                |\n",
    " |:--------------------:|:--------------------------------------------------------------------------------:|\n",
    " |     `Imputation`     |                    fills out missing values in data                    |\n",
    " |   `Discretization`   |                groups a feature in some logical fashion into bins                |\n",
    " |`Categorical Encoding`|encodes categorical features into numerical values|\n",
    " |  `Feature Splitting` |splits a feature into parts|\n",
    " |   `Feature Scaling`  |handles the sensitivity of ML algorithms to the scale of input values| \n",
    " |`Feature Expansion`|derives new features from existing ones|\n",
    " | `Log Transformation` |deals with ill-behaved (skewed of heteroscedastic) data       |\n",
    " |   `Outlier Handling` |takes care of unusually high/low values in the dataset|\n",
    " | `RBF Transformation` |uses a continuous distribution to encode ordinal features|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf171688-6f9c-4762-8f80-0932febd83fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature engineering best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa0d7b-3052-4aab-a398-c5eaa4cf76bc",
   "metadata": {},
   "source": [
    "#### 1. **Split Dataset** into Train and Test sub-samples as early as possible\n",
    "\n",
    "While this process is flexible‚Äîfor example, you can remove NaNs from the entire dataset before filling‚Äîit's generally a better practice, in the interest of good machine learning habits, to perform this step **after splitting**. If you remove or impute missing values before splitting, information from the test set could influence the training process, leading to overly optimistic performance estimates.\n",
    "\n",
    "#### 2. **Feature Engineering** Includes any pre-processing techniques, such as:\n",
    "\n",
    "- Dropping missing values\n",
    "- Converting strings or non-numeric values into numeric values\n",
    "- Combining features\n",
    "- Creating new features\n",
    "\n",
    "#### 3. **Feature Engineer Test Data** the same way as train data\n",
    "\n",
    "Make sure to process the test data in the same way as training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbc6c5",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b385e939",
   "metadata": {},
   "source": [
    "## Example: Penguin Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e60e499",
   "metadata": {},
   "source": [
    "#### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization stack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# miscellaneous\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997025c0",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaaf129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/penguins.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0057726",
   "metadata": {},
   "source": [
    "#### Quick Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd3f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f97f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc3d77",
   "metadata": {},
   "source": [
    "#### Features and Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    'bill_length_mm',\n",
    "    'bill_depth_mm',\n",
    "    'flipper_length_mm'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'species',\n",
    "    'island',\n",
    "    'sex'\n",
    "]\n",
    "\n",
    "features = numerical_features + categorical_features\n",
    "\n",
    "target_variable = 'body_mass_g'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9c9cc-7158-47ca-9680-42f52f517f0b",
   "metadata": {},
   "source": [
    "#### Feature-Target separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b785ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature matrix \n",
    "X = df[features]\n",
    "\n",
    "# Target column\n",
    "y = df[target_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4b01f",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef96c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42, shuffle=True)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed588ec",
   "metadata": {},
   "source": [
    "This code snippet **splits** the dataset into training and testing sets, where 25% of the data is reserved for testing. The `random_state` parameter ensures that the **split** is reproducible; the same random split will occur each time the code is run. The `shuffle` ensures that the data is shuffled before the split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553730c",
   "metadata": {},
   "source": [
    "For teaching purposes, we'll demonstrate how to add a **validation set** in addition to the usual **train-test split**. The validation set is important to properly **evaluate** and **fine-tune** a model before final testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10851f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d361297",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48174b34",
   "metadata": {},
   "source": [
    "For **Exploratory Data Analysis (EDA)**, you should concatenate `X_train` and `y_train`. This allows you to analyze also the relationship between features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train is a DataFrame and y_train is a Series\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "print(\"Combined train data shape:\", df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646240e7",
   "metadata": {},
   "source": [
    "**Show Some Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_train,corner=True,hue='island');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1012b5",
   "metadata": {},
   "source": [
    "The above **pairplot** shows both distribution of single variables and the relationships between two variables. \n",
    "\n",
    "1. **Scatter Plots**:\n",
    "   - Each plot below the diagonal shows how two variables relate to each other. For example, in the plot where 'flipper_length_mm' meets 'bill_length_mm', each point represents those two measurements for one observation.\n",
    "\n",
    "2. **Histograms/Density Plots**:\n",
    "   - The plots along the diagonal show how frequently different values occur for a single variable. For instance, the plot for 'bill_depth_mm' displays the distribution of bill depths among all observations.\n",
    "\n",
    "3. **Color**:\n",
    "   - The colors represent different categories, in this case, categorized by 'island'. This helps to quickly see if measurements vary noticeably by island, with each color representing a different island.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fd22e",
   "metadata": {},
   "source": [
    "One can create also other pairplots considering as the extra dimension `hue` the sex variable or the species variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570debe",
   "metadata": {},
   "source": [
    ">**Note** that the **indices** in the DataFrame shown above appear in a **random order**, a result of **shuffling** prior to the data split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30472cb",
   "metadata": {},
   "source": [
    "#### Issue with the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fab39",
   "metadata": {},
   "source": [
    "**Many models cannot handle** missing values, **categorical features** with non-numeric values, or **metric features** with varying magnitudes. Proper data preprocessing is essential to prepare the data for these models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca40f908",
   "metadata": {},
   "source": [
    "**Check Missing Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe63698",
   "metadata": {},
   "source": [
    "First, let's take a look at what's missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a69cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b5bf1a",
   "metadata": {},
   "source": [
    "`isna()`and `isnull()` are identical methods that produce a boolean mask where True is a missing value. If we want to turn this into a useful view, we can filter using these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d1ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the rows with NaN\n",
    "df_train.loc[df_train.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d61c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check missing values graphically\n",
    "plt.figure(figsize=(7,5), dpi=100)\n",
    "sns.heatmap(df_train.isna());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e363043b",
   "metadata": {},
   "source": [
    "So, looking at either of these, we can see two missing values in both `bill_length_mm` and `flipper_length_mm` columns and six in the `sex` column. How do we deal with them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530645b",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c201c5",
   "metadata": {},
   "source": [
    "## 1. Imputation - Filling in the Blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d5c29",
   "metadata": {},
   "source": [
    "#### What can we do with missing information?\n",
    "There are __few strategies__:\n",
    "\n",
    "- __Drop__:\n",
    "    + rows with missing values\n",
    "    + columns with a lot of missing values\n",
    "- __Fill with a value__:\n",
    "    + __mean__/__median__/__mode__ of a column\n",
    "    + __interpolate__ / __back fill__ / __forward fill__\n",
    "    + __mean__/__median__/__mode__ of a group\n",
    "\n",
    "- With `pandas`: \n",
    "    - `df.isna()`: checks for NaNs, then do a sum or a heatmap\n",
    "    - `df.dropna()`: drop NaNs\n",
    "    - `df.fillna()`: fill NaNs\n",
    "\n",
    "One would to use `inplace=True` in these examples to modify the DataFrame directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f72f5-87a8-4ac3-98d5-1fa1ede70d23",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eeebbd-a5b6-44ca-b602-10b5cca6aab0",
   "metadata": {},
   "source": [
    "#### 1.1 `SimpleImputer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72c953",
   "metadata": {},
   "source": [
    "We can use the scikit-learn  <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\">`SimpleImputer()`</a>  to quickly impute the missing data in a column. There are three strategies available:\n",
    "\n",
    " * `strategy = 'mean'` - **the default option**, numeric only\n",
    " \n",
    " * `strategy = 'median'` - numeric only\n",
    " \n",
    "* `strategy = 'most_frequent'` - mode, numeric or categorical\n",
    " \n",
    " * `strategy = 'constant'` - needs additional arg `fill_value`, numeric or categorical\n",
    "\n",
    "If your missing values aren't NaN (i.e. - None, 0, 999, \"badvalue\", etc), you may need to use the `missing_value` argument to let it know what to look for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c72c69-e925-45ef-96ec-5b4d8e069acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a68d63-e81d-4bc4-96d4-72dd57e749ed",
   "metadata": {},
   "source": [
    "**Sex Column Imputation with the most frequent value**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['sex'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa04cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiating a SimpleImputer object\n",
    "sex_imputer = SimpleImputer(strategy='most_frequent')#.set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a498d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the variable imputer on the 'sex' column training data\n",
    "sex_imputer.fit(X_train[['sex']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cebb5b",
   "metadata": {},
   "source": [
    "`.fit` teaches the imputer what to insert. In this example the imputer scans through the `sex` column in the training dataset (`X_train`) to determine the most frequent value in that column. After `fitting`, the imputer will inernally store the mode of that column in its `statistics_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436be49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ea8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the transformation to the 'sex' column of the training data using the pre-fitted imputer.\n",
    "sex_imputed_train = sex_imputer.transform(X=X_train[['sex']])\n",
    "sex_imputed_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff60f4a",
   "metadata": {},
   "source": [
    "The `transform` method uses a pre-trained imputer to fill in the `missing values` in the `sex` column. By default, this method returns a numpy array rather than a pandas DataFrame. If you need to convert this numpy array back into a DataFrame, it's important to ensure that the index of the new DataFrame aligns with the original one to maintain data consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The long way to the dataframe  \n",
    "sex_imputed_df_train = pd.DataFrame(data=sex_imputed_train, columns=sex_imputer.get_feature_names_out(), index=X_train.index)\n",
    "sex_imputed_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_imputed_df_train.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d33f1",
   "metadata": {},
   "source": [
    "**How should we impute missing values in the test data?**\n",
    "\n",
    "When imputing missing values - or applying other data transformations to the **test data**, it is important to **avoid using any information outside of the training data**. This helps us to avoid **data leakage** during the model building process. Therefore, you should apply only those transformations to the test data that are based on parameters established from the training data. Thus, in the context of scikit-learn, we use only the __transform method__ of the imputer or other transformation tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af796b5",
   "metadata": {},
   "source": [
    "üö®üö®üö®**Very Important**üö®üö®üö®\n",
    "\n",
    "As shown in the [machine learning workflow](../machine_learning_workflow.md#the-machine-learning-workflow) it is good practice to unlock and transform the test data only at the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_imputed_test = sex_imputer.transform(X_test[['sex']])\n",
    "sex_imputed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_imputed_df_test = pd.DataFrame(data=sex_imputed_test,columns=sex_imputer.get_feature_names_out(), index=X_test.index)\n",
    "sex_imputed_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2916d4-a1a2-4587-8951-4b257108b34d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa1dd5-bb7c-4c3b-84c9-8ecdca9861e5",
   "metadata": {},
   "source": [
    "**Imputation of 'Flipper Length' and 'Bill Depth' Columns Using Median Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b58b6",
   "metadata": {},
   "source": [
    "We will follow the same steps we used for the 'sex' column to address missing values in the 'flipper length' and 'bill depth' columns. But in this case we will use the **median imputation** strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e3a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a SimpleImputer object\n",
    "flipper_bill_imputer = SimpleImputer(strategy='median').set_output(transform='pandas')\n",
    "flipper_bill_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf9f88",
   "metadata": {},
   "source": [
    "The `set_output()` method sets the default otput of the `transform()` methods to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec965f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the variable imputer on the flipper_length_mm' and 'bill_depth_mm' columns of the training data\n",
    "flipper_bill_imputer.fit(X_train[['flipper_length_mm','bill_depth_mm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd6528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the median parameters stored in the imputer after the fit\n",
    "flipper_bill_imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e102285",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['flipper_length_mm','bill_depth_mm']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the transformation to the flipper_length_mm' and 'bill_depth_mm' columns of the training data using the pre-fitted imputer.\n",
    "flipper_bill_imputed_df_train = flipper_bill_imputer.transform(X_train[['flipper_length_mm','bill_depth_mm']])\n",
    "flipper_bill_imputed_df_train.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbeef8-a852-4404-b818-96af8e762714",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f683a3",
   "metadata": {},
   "source": [
    "## 2. Categorical encoding - Replacing categories with numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece663a",
   "metadata": {},
   "source": [
    "Most algorithms aren't capable of handling strings, but there are some helpful tools to convert them into integers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd3eca-72fc-456f-b97e-0c3467c3a0d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 `OneHotEncoder` for **nominal variables** (categories without inherent order)\n",
    "We can use the scikit-learn <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=onehotencoder#sklearn.preprocessing.OneHotEncoder\">`OneHotEncoder()`</a> to get dummy values for our categorical data. It converts each category within a column into a separate binary column. Each category is represented by a **1** in its respective column for instances where it appears, and **0** where it does not. There are two options for chosing which columns to drop to avoid perfect collinearity - a common statistical issue for some models:\n",
    " \n",
    " * `drop = 'first'` - drops the first category in each feature. If there's only one category, it will drop the feature altogether.\n",
    " \n",
    " * `drop = 'if_binary'` - will only drop a category if the feature is binary (i.e. - yes/no, on/off, etc). Features with one or more than two categories will remain untouched.\n",
    "\n",
    "After transforming our dataset, we can use `get_feature_names_out()` to get an array of feature names to label the columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4cb08-f4d6-41fd-ad86-a65126e5857a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087529ff",
   "metadata": {},
   "source": [
    "What are the **unique categories** in the `species` column/feautre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f11c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiating a OneHotEncoder object\n",
    "species_ohencoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bbca3",
   "metadata": {},
   "source": [
    "+ The `handle_unknown` parameter tells the encoder how to deal with categories in new data that weren't present during its initial fitting.\n",
    "+ When you set `sparse_output=True`, the encoder returns the transformed data as a **sparse matrix** instead of a dense numpy array.\n",
    "  + **Dense Matrix**\n",
    "    + Stores all values explicitly, including zeros\n",
    "    + Very memory-intesive when the matrix has a lot of zeros\n",
    "  + <a href=\"https://en.wikipedia.org/wiki/Sparse_matrix#:~:text=In%20numerical%20analysis%20and%20scientific,of%20the%20elements%20are%20zero.\">**Sparse Matrix**</a>\n",
    "    + Stores only the locations and values of non-zero elements\n",
    "    + Reduces memory usage\n",
    "    + Speeds up operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the variable encoder on the 'species' column of the training data\n",
    "species_ohencoder.fit(X=X_train[['species']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a121356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique categories of the train species column learnt and stred in the encoder\n",
    "species_ohencoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde96cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new columns/featureeaure of the categories\n",
    "species_ohencoder.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ccb27",
   "metadata": {},
   "source": [
    "**Note that `species_Adelie` has been dropped**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_encoded_train_sparse = species_ohencoder.transform(X=X_train[['species']])\n",
    "species_encoded_train_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218fd856",
   "metadata": {},
   "source": [
    "We have a **Compressed Sparse Row (CSR)** sparse matrix with dimensions **(nrows, ncols) = (256, 2)**, where most of the entries are zero. This matrix efficiently stores only the 149 non-zero entries, as floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a sparse matrix to a dense matrix\n",
    "species_encoded_train_dense = species_encoded_train_sparse.todense()\n",
    "species_encoded_train_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177db88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \n",
    "species_encoded_train_df = pd.DataFrame(data=species_encoded_train_dense, columns=species_ohencoder.get_feature_names_out(), index=X_train.index)\n",
    "species_encoded_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the encoded species columns for two penguins\n",
    "species_encoded_train_df.loc[[24,323]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bef659",
   "metadata": {},
   "source": [
    "**How to read the üëÜüèΩ DataFrame**?\n",
    "\n",
    "+ Observation (the penguin) n 24 does not belong neither to the species `Chinstrap` not `Gentoo` but `Adelie`\n",
    "+ Observation (the penguin) n 323 belong to the species `Gentoo` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711fe22-1b5f-4076-a4eb-e33604010c1c",
   "metadata": {},
   "source": [
    "### 2.2 `OrdinalEncoder` for **Ordinal Variables** (Categories with Inherent Order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726605f",
   "metadata": {},
   "source": [
    "We can use the scikit-learn <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\">`OrdinalEncoder()`</a> to encode ordinal categories into meaningful numeric values. This encoder is particularly useful for categories that have a natural ordered relationship. Here are some key points:\n",
    "\n",
    "* `categories` - specifies the order of categories explicitly if the default lexical order is not desired.\n",
    "\n",
    "* `dtype` - the data type of the output (default is `float64`).\n",
    "\n",
    "* `handle_unknown` - decides how to handle unknown categories that appear during transformation:\n",
    "  \n",
    "  * `handle_unknown='error'` - **the default option**, throws an error if an unknown category is encountered.\n",
    "  \n",
    "  * `handle_unknown='use_encoded_value'` - allows assigning a specific integer for unknown categories with additional arg `unknown_value`.\n",
    "\n",
    "If your categories have a meaningful order, specify this in the `categories` argument to ensure the encoding respects the ordinal nature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86299207",
   "metadata": {},
   "source": [
    "As in the penguins dataset we don't have a feature with ordinal categories, let's consider a simple case involving an ordinal feature: \"Education Level\". The categories, listed in order of their educational achievement, are:\n",
    "\n",
    "+ High School\n",
    "+ Bachelor's\n",
    "+ Master's\n",
    "+ Doctorate\n",
    "\n",
    "In ordinal encoding, each category is assigned a unique integer based on its order.\n",
    "Here is how you can map these categories:\n",
    "+ High School --> 0\n",
    "+ Bachelor's --> 1\n",
    "+ Master's  --> 2\n",
    "+ Doctorate --> 3\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034493f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [['Bachelor\\'s'], ['High School'],  ['Doctorate'], ['Master\\'s'], ['Doctorate']]\n",
    "df_train_data_ = pd.DataFrame(data=train_data, columns=['education_level'])\n",
    "df_train_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the encoder object\n",
    "edu_encoder = OrdinalEncoder(categories=[['High School', 'Bachelor\\'s', 'Master\\'s', 'Doctorate']],dtype=int).set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3335e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the variable encoder on the 'education_level' column of the training data\n",
    "edu_encoder.fit(df_train_data_[['education_level']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51713a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned and stored categories by the encoder\n",
    "edu_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'education_level' column in the train data\n",
    "edu_encoded_df_train = edu_encoder.transform(df_train_data_)\n",
    "edu_encoded_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdabe31",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f189b",
   "metadata": {},
   "source": [
    "`OrdinalEncoder()` and `OneHotEncoder()` turn categorical data into an integer\n",
    "\n",
    "`OrdinalEncoder()` results in a single column\n",
    "\n",
    "`OneHotEncoder()` results in multiple columns\n",
    "\n",
    "`drop_first` removes a column from your dummy frame to avoid **perfect collinearity**, especially when using a regression model. There is some discussion of why multicollinearity is <a href = \"https://towardsdatascience.com/multicollinearity-why-is-it-bad-5335030651bf\">a problem</a> and <a href = \"https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn\">why it might not be that bad</a>. The arguments are interesting, but as a general rule, when using regression models it's best to avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ec0db",
   "metadata": {},
   "source": [
    "\n",
    "- `LabelEncoder`  similar to [factorize()](https://pandas.pydata.org/docs/reference/api/pandas.factorize.html) in Pandas \n",
    "- `OneHotEncoder` similar to [get_dummies()](https://pandas.pydata.org//docs/reference/api/pandas.get_dummies.html) in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22df328-46fa-4977-be9f-ebb1e430d62c",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c25c3",
   "metadata": {},
   "source": [
    "## 3. Discretization (Binning) - Splitting scalars into categories\n",
    "Breaking a continuous variable into buckets can lead to some effects:\n",
    "+ it can reduce the model's sensitivity to minor fluctuations and noise in the data, reducing the risk of overfitting\n",
    "+ it can bring loss of important details in the data if the bins width are not properly choosen, leading to higher risk of underfitting\n",
    "+ it makes linear models non-linear:\n",
    "  + [regression example from scikit-learn](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization.html)\n",
    "  + [classification example from scikit-learn](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_classification.html#sphx-glr-auto-examples-preprocessing-plot-discretization-classification-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d581b1e",
   "metadata": {},
   "source": [
    "### `KBinsDiscretizer()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f777e4",
   "metadata": {},
   "source": [
    "We can use <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html\">`KBinsDiscretizer()`</a> to turn a set of scalars into bins, and then one-hot encode these bins in a single step. A few parameters to keep in mind:\n",
    " \n",
    "* `n_bins` - choose how many bins to generate, default = 5.\n",
    " \n",
    "* `strategy = 'quantile'` - **default option**, generate bins of equal population.\n",
    " \n",
    "* `strategy = 'uniform'` - generate bins of equal width.\n",
    "  \n",
    "* `encode = 'onehot' ` - **default option**, encode the transformed result with one-hot encoding and return a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe21948",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=X_train, x='bill_length_mm',bins=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate  KBinsDiscretizer object\n",
    "bill_length_binner = KBinsDiscretizer(n_bins = 5, encode='onehot-dense', strategy='quantile').set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87568fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the variable binner on the 'bill_length_mm' column of the training data\n",
    "bill_length_binner.fit(X_train[['bill_length_mm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce995c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access and print the bin edges for the 'bill_length_mm' feature\n",
    "bin_edges = bill_length_binner.bin_edges_\n",
    "print(\"Bin edges for 'bill_length_mm':\", bin_edges[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853374a6",
   "metadata": {},
   "source": [
    "The bin edges represents the `quintiles`. They divides the the data into 5 more-or-less equal parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483138e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the quintiles\n",
    "X_train[['bill_length_mm']].quantile(q=[0.,0.2,0.4,0.6,0.8,1.0]).rename({'bill_length_mm':'bill_quintiles'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the bill length distribution using the learnt bin edges \n",
    "sns.displot(data=X_train, x='bill_length_mm',bins=bin_edges[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2195bec",
   "metadata": {},
   "source": [
    "As shown in the plot above the heights of the bins are more-or-less equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb420ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform 'bill_length_mm' using the quintile bins defined in 'bill_length_binner'\n",
    "bill_length_binned_df_train = bill_length_binner.transform(X_train[['bill_length_mm']])\n",
    "bill_length_binned_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ddc69",
   "metadata": {},
   "source": [
    "The following table shows you the mapping between one-hot-encoded categories and binned values:\n",
    "| Category | Bin Values     |\n",
    "|----------|----------------|\n",
    "| 0        | 32.1 - 38.6   |\n",
    "| 1        | 38.6 - 42.3|\n",
    "| 2        | 42.3 - 46.2|\n",
    "| 3        | 46.2 - 49.5|\n",
    "| 4        | 49.5 - 59.6|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa25bb",
   "metadata": {},
   "source": [
    "**How to read the üëÜüèΩ transformed DataFrame**?\n",
    "+ The transformed DataFrame categorizes each penguin's `bill_length_mm` into one of five quintile categories (0 through 4). Each category corresponds to a specific range of bill lengths\n",
    "+ Observation (the penguin) n 24 is labeled as '0', this indicates that the the bill length falls in the range from the miniimum 32.1 to the first quintile 38.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce572d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bill_length value for the observation 24\n",
    "X_train[['bill_length_mm']].loc[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2786190",
   "metadata": {},
   "source": [
    "- `KBinsDiscritizer(strategy='quantile)`  similar to [qcut()](https://pandas.pydata.org/docs/reference/api/pandas.qcut.html) in Pandas \n",
    "- `KBinsDiscritizer(strategy='uniform)` similar to [cut()](https://pandas.pydata.org/docs/reference/api/pandas.cut.html) in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c9949f",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4383a-7a8c-4f5f-90b4-9f08d3339fa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Numerical Feature Scaling - Normalizing Data Ranges\n",
    "\n",
    "The goal of feature scaling is to transform numerical features to be on a similar scale.\n",
    "\n",
    "For example, consider the following two features:\n",
    "+ `income` spans from 20_000 to 100_000 euro:\n",
    "+ `age` spans from 20 to 100 years\n",
    "  \n",
    "Without scaling, the income feature would disproportionately influence [any distance-based algorithms](https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35/) because of its larger range of values compared to age.\n",
    "\n",
    "By applying feature scaling, both income and age can be adjusted so that they **equally contribute to the model's learning**, thereby reflecting true feature importance and subsequently improving the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740cf11",
   "metadata": {},
   "source": [
    "### 4.1 Min-Max Scaling \n",
    "\n",
    "It transforms all values of a numerical feature to a fixed range, typically between 0 and 1, by subtracting the minimum value and dividing by the range as shown in the formula below:\n",
    "\n",
    "$$\\large X_{scaled} = \\large \\dfrac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "After transformation:\n",
    "+ Features are constrained to a specific range (e.g., 0 to 1) \n",
    "  + Minimum value becomes 0, maximum value become 1, \n",
    "+ The original relationship between data points is preserved\n",
    "  + Everything in between 0 and 1 is proportionally distributed\n",
    "+ Outliers are not handled well.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b310b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the scaler\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0,1)).set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler to the columns 'bill_length_mm' and 'flipper_length_mm' of the training data\n",
    "min_max_scaler.fit(X=X_train[['bill_length_mm','flipper_length_mm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16dffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnt Maximum values for each feature on the train data\n",
    "print(\"Max values:\", min_max_scaler.data_max_)\n",
    "\n",
    "# Learnt Minimum values for each feature on the train data\n",
    "print(\"Min values:\", min_max_scaler.data_min_)\n",
    "\n",
    "# Learnt Range for each feature on the train data\n",
    "print(\"Range:\", min_max_scaler.data_range_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d5b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['bill_length_mm','flipper_length_mm']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "min_max_scaled_bill_fli_df_train = min_max_scaler.transform(X=X_train[['bill_length_mm','flipper_length_mm']])\n",
    "min_max_scaled_bill_fli_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaled_bill_fli_df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507fea8c",
   "metadata": {},
   "source": [
    "**Note** that the **min** and **max** value for each features are respectively 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38375e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2,ncols=2)\n",
    "\n",
    "# Plotting unscaled data\n",
    "sns.histplot(data=X_train, x='bill_length_mm', ax=ax[0, 0], kde=True)\n",
    "ax[0, 0].set_title('Unscaled Bill Length')\n",
    "\n",
    "sns.histplot(data=X_train, x='flipper_length_mm', ax=ax[0, 1], kde=True)\n",
    "ax[0, 1].set_title('Unscaled Flipper Length')\n",
    "\n",
    "# Plotting scaled data\n",
    "sns.histplot(data=min_max_scaled_bill_fli_df_train, x='bill_length_mm', ax=ax[1, 0], kde=True)\n",
    "ax[1, 0].set_title('Scaled Bill Length')\n",
    "\n",
    "sns.histplot(data=min_max_scaled_bill_fli_df_train, x='flipper_length_mm', ax=ax[1, 1], kde=True)\n",
    "ax[1, 1].set_title('Scaled Flipper Length')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1195e492",
   "metadata": {},
   "source": [
    "As shown in the visualization above. The underlying pattern of the data remains preserved after min-max scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b67b4a",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Standard Scaling\n",
    "\n",
    "It transforms the values of a numerical feature so that they have a mean of zero and a standard deviation of one. This is accomplished by subtracting the mean of the feature from each value and then dividing by the standard deviation, as shown in the formula below:\n",
    "\n",
    "$$\\large X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ is the mean of the feature values.\n",
    "- $\\sigma$ is the standard deviation of the feature values.\n",
    "\n",
    "**After transformation:**\n",
    "+ **Centered Data**: The mean of the transformed data is 0\n",
    "+ **Unit Variance**: The standard deviation becomes 1, which means that feature variance is normalized.\n",
    "+ **Preserves Relationships**: The original relationships between variables are maintained, similar to Min-Max scaling.\n",
    "+ **Handling Outliers**: Standard scaling is less sensitive to outliers than Min-Max scaling because it does not compress the data into a fixed range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b6630c",
   "metadata": {},
   "source": [
    " **[`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae213019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6570d",
   "metadata": {},
   "source": [
    "### 4.3 Robust Scaling\n",
    "It transforms the values of a numerical feature by subtracting the median and then dividing by the interquartile range (IQR), as shown in the formula below:\n",
    "\n",
    "$$\\large X_{\\text{robust}} = \\frac{X - \\text{median}(X)}{\\text{IQR}}$$\n",
    "\n",
    "Where:\n",
    "- **median(X)** is the median of the feature values.\n",
    "- **IQR** is the interquartile range, which is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) of the feature values.\n",
    "\n",
    "After transformation:\n",
    "+ **Centered and Scaled Data**: The median of the transformed data becomes 0. The IQR used as the scaler normalizes the feature variance.\n",
    "+ **Handling Outliers**: Since the median and IQR are less affected by outliers than the mean and standard deviation, this scaling method is much better suited for datasets with outliers.\n",
    "+ **Preserving Relationships**: Like other scaling methods, robust scaling maintains the original relationships between variables that are not outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fbc323-096a-4a80-87df-d00da360ad29",
   "metadata": {},
   "source": [
    "**[`RobustScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66411cb4",
   "metadata": {},
   "source": [
    "### 4.4 Log Scaling\n",
    "Log scaling is a non-linear transformation that uses the logarithmic function to reduce the range and variation of data values.\n",
    "\n",
    "**Log transformation** applies a logarithmic scale to the values of a numerical feature, typically using the natural logarithm (base e), although any logarithm base can be used depending on the data and the specific needs. The transformation is given by the formula:\n",
    "\n",
    "$$\\large X_{\\text{log}} = \\log(X)$$\n",
    "\n",
    "Where:\n",
    "- **log()** denotes the logarithmic function, which could be to any base.\n",
    "\n",
    "**After transformation:**\n",
    "+ **Reduces Scale Differentials**: the scale of high magnitude values is more significantly compressed than those of low magnitude.\n",
    "+ **Handling Skewness**: Converts a skewed distribution into one that is more uniform.\n",
    "+ **Stabilizing Variance**: Variance near larger values is reduced more than variance near smaller values, which stabilizes variance across the dataset.\n",
    "+ **Reducing Impact of Outliers**: Outliers that are far from the majority of data points become less dominating after log transformation due to the compression effect at higher value ranges.\n",
    "\n",
    "**Considerations:**\n",
    "- Log transformation can only be applied to positive values. For datasets containing zero or negative values, a constant may be added to each value before applying the log to shift all data into the positive domain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00739530",
   "metadata": {},
   "source": [
    "While **scikit-learn** itself does not provide a direct transformer for log transformations, you can easily implement log transformation using a custom transformer with the help of [**FunctionTransformer**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html). This allows you to apply any function, including a logarithmic transformation, to your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a640eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define the log transformation function\n",
    "# Adding a constant to avoid taking log of zero\n",
    "def log_transformation(X, c):\n",
    "    return np.log(X + c)\n",
    "\n",
    "def inverse_log_transformation(Xlog, c):\n",
    "    return np.exp(Xlog) - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transformer using the log_transform function\n",
    "log_transformer = FunctionTransformer(func=log_transformation, inverse_func=inverse_log_transformation, kw_args={'c':1}, inv_kw_args={'c':1}).set_output(transform='pandas')\n",
    "                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the transformer to the 'flipper_length_mm' column of X_train\n",
    "log_transformer.fit(X=X_train[['flipper_length_mm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data using the fitted transformer\n",
    "log_transformed_flip_df_train = log_transformer.transform(X_train[['flipper_length_mm']])\n",
    "log_transformed_flip_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2,ncols=1)\n",
    "\n",
    "# Plotting unscaled data\n",
    "sns.histplot(data=X_train, x='flipper_length_mm', ax=ax[0], kde=True)\n",
    "ax[0].set_title('Unscaled Flipper Length')\n",
    "\n",
    "sns.histplot(data=log_transformed_flip_df_train, x='flipper_length_mm', ax=ax[1], kde=True)\n",
    "ax[1].set_title('Scaled Flipper Length')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e203c7d6",
   "metadata": {},
   "source": [
    "#### Readings on Scaling\n",
    "+ [Compare the effect of different scalers on data with otliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)\n",
    "+ [Importance of feature scaling](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d94e0-e830-4ac8-b4bf-f4fa5803cd3e",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3082a3cd-4572-48f8-b847-033778a57fe7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Feature Expansion\n",
    "### `PolynomialFeatures`\n",
    "We can use the scikit-learn [`PolynomialFeatures()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to add polynomial or interaction features to our dataset. This transformer is particularly useful for capturing interactions between features in a nonlinear model. Here are some key points:\n",
    "\n",
    "* `degree` - specifies the degree of the polynomial features (default is 2). For example, for a single feature \\(X\\), if `degree=2`, it generates \\(X, X^2\\).\n",
    "\n",
    "* `interaction_only` - if set to `True`, this will produce features that are the product of distinct input features. For example, if two features are \\(X_1\\) and \\(X_2\\), it will generate \\(X_1 \\times X_2\\) but not \\(X_1^2\\) or \\(X_2^2\\).\n",
    "\n",
    "* `include_bias` - decides whether to include a bias column (the feature column consisting of ones). This can be set to `False` if a bias is already handled or not required in the model.\n",
    "\n",
    "üö®**Very important**üö®:\n",
    "+ **you scale your features first and then apply polynomial feature expansion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb9dc3-083f-4123-8c66-4cd5010a289f",
   "metadata": {},
   "source": [
    "### 5.1 `Polynomial Terms`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49fc03-8aab-4ad5-8a7d-a845d06b9c8f",
   "metadata": {},
   "source": [
    "- Additional features obtained by an existing feature to some power\n",
    "- Non-linear relationships can be modelled\n",
    "- For some feature x, consider the model: \n",
    "\n",
    "$$\n",
    "y = a_0 + a_1x + a_2x^2 +\\ldots+\\epsilon\n",
    "$$\n",
    "\n",
    "- Potential improvement of model accuracy, but increased risk of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213b4d2-fb46-47c2-b51c-861e9c2c6e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e134b-9005-49fa-89c6-7d886ad0a1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the polynomial features object\n",
    "poly_expansion = PolynomialFeatures(degree = 2, interaction_only = False, include_bias = False).set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d800913-9b76-404c-904c-5a0f6d0e6ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poly_expansion.fit(min_max_scaled_bill_fli_df_train[['bill_length_mm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e18f4-926b-4cf7-aaf0-24188e47fa9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expanded_scaled_bill_length_df_train = poly_expansion.transform(min_max_scaled_bill_fli_df_train[['bill_length_mm']])\n",
    "expanded_scaled_bill_length_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b027e2-cc77-4c80-b651-b1116d3d50e9",
   "metadata": {},
   "source": [
    "### 5.2 `Interaction Terms`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e918e7-ee15-4b07-b911-b13083faeb0a",
   "metadata": {},
   "source": [
    "- For multiple initial features, there could be *interactions* (cross-polynomial terms)\n",
    "- For 2 features, $x_0$ and $x_1$ for example, a 2nd-degree polynomial may contain:\n",
    "\n",
    "$$\n",
    "1,~x_0,~x_1,~x_0^2,~x_0x_1,~x_1^2\n",
    "$$\n",
    "\n",
    "- Each of the terms gets their own coefficient in a regression model\n",
    "- Polynomial preprocessing function with `interaction_only = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e29726-eaf9-47ce-8c61-f720d5439b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the polynomial features object\n",
    "poly_expansion = PolynomialFeatures(degree = 2, interaction_only = False, include_bias = False).set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3d234",
   "metadata": {},
   "source": [
    "Since PolynomialFeatures doesn‚Äôt support NaNs, we must preprocess the data before calling .fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131a856",
   "metadata": {},
   "source": [
    "Let's check for missing values first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98946b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(min_max_scaled_bill_fli_df_train).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8f5a4",
   "metadata": {},
   "source": [
    "There are two missing values in the flipper_length variable. To handle this, we could apply various strategies you learned before, such as filling, imputing, or dropping. Since the data loss is minimal, let's simply drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missings in data frame\n",
    "min_max_scaled_bill_fli_df_train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missings\n",
    "np.isnan(min_max_scaled_bill_fli_df_train).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_expansion.fit(X=min_max_scaled_bill_fli_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_scaled_bill_fli_length_df_train = poly_expansion.transform(min_max_scaled_bill_fli_df_train)\n",
    "expanded_scaled_bill_fli_length_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093bfec3",
   "metadata": {},
   "source": [
    "**Note** that the column `bill_length_mm flipper_length_mm` represents the interaction term. If `interaction_only = True` the quadratic features `bill_length_mm^2` and `flipper_length_mm^2` will not be present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d93e5a",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014dba9",
   "metadata": {},
   "source": [
    "## Bonus üå∂Ô∏èüå∂Ô∏èüå∂Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5bb7c",
   "metadata": {},
   "source": [
    "\n",
    "### Spiced Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98eab8d",
   "metadata": {},
   "source": [
    "Previously, we looked at basic imputation methods that use simple statistics like the **mean** or **median** from the same column with missing values.\n",
    "As always, there is more to dicover. There are also more advanced imputation techniques that consider relationships between different features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343180c7",
   "metadata": {},
   "source": [
    "#### Group-wise Mean/Median/Mode Imputation:\n",
    "Imputes missing values based on the mean, median, or mode calculated within subgroups of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92d5e0",
   "metadata": {},
   "source": [
    "To build a group-wise imputer that follows the scikit-learn interface of `.fit()` and `.transform()`, we will create a custom class based  on scikit-learn's `BaseEstimator` and `TransformerMixin` classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd20bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupMeanImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_column, target_columns):\n",
    "        # Initialize with the name of the group column and a list of target columns\n",
    "        self.group_column = group_column\n",
    "        self.target_columns = target_columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate the mean for each target column within each group\n",
    "        self.means_ = {}\n",
    "        # Calculate the unique categories in the group  column\n",
    "        self.categories_ = X[self.group_column].unique()\n",
    "        for column in self.target_columns:\n",
    "            self.means_[column] = X.groupby(self.group_column)[column].mean().to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Apply the learned means_ to fill in missing values for each target column\n",
    "        X = X.copy()\n",
    "        for column in self.target_columns:\n",
    "            # Get the group means_ for the current column\n",
    "            for category in self.categories_:\n",
    "                group_means = self.means_[column][category]\n",
    "                # Fill missing values for the current column based on its group mean\n",
    "                X.loc[:, column] = X.groupby(self.group_column)[column].fillna(group_means)\n",
    "            \n",
    "        return X[self.target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e778f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the group-wise imputer\n",
    "group_mean_imputer = GroupMeanImputer(group_column='island', target_columns=['bill_depth_mm','flipper_length_mm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f2b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the imputer \n",
    "group_mean_imputer.fit(X=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_mean_imputer.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7af3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_mean_imputer.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_mean_imputer.transform(X=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1b734",
   "metadata": {},
   "source": [
    "### Model Imputation:\n",
    "+ Use a model to predict the missing values based on other variables\n",
    "+ For iterative imputation in scikit-learn there is  [IterativeImputer()](https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9841ed1",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN) Imputation:\n",
    "+ Imputes missing entries based on the k-nearest neighbors found by measuring distance from other points.\n",
    "+ In scikit-learn there is [KNNImputer()](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
